import pymysql
import pandas as pd
import nltk
import string
import re
import sys
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

class usesql:
    
    def _init_(self,conn,cursor):
        self.conn = conn
        self.cursor = cursor


    def getdata(self,sql):   # 从mysql里读取数据
        self.conn = pymysql.connect(
            host = 'localhost',
            port = 3306,
            user = 'Leslie',
            password = 'hsh56912341',
            db = 'Yelp',
            charset = 'utf8'
        )
        self.cursor = self.conn.cursor()
        sql='select review from review21;'
        readfile = self.cursor.execute(sql)
        result = readfile.fetchmany(10)   #测试的时候先用fetchmany（10）,正式运行时再改成fetchall
        return list(result)
        

    def output(self,word):
        self.cursor = self.conn.cursor()
        df = pd.DataFrame (list(word))
        df.to_sql(         #将输出结果转为dataframe格式存入mysql，表的名称为测试阶段暂定，正式名称最后命名为reviewdata
            name='firstest',
            conn=conn,
            if_exists='append',
            index=False,
        )
        self.cursor.close()
        self.conn.close()
        

class prepare:

    def _init_(self,word,relativity,id,ordinal):
        self.word = word
        self.relativity = relativity
        self.id = id
        self.ordinal = ordinal
        
    def sentoken(self,data):   #分句
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        self.sent = sent_tokenizer.tokenize(str(data))
        return self.sent
        
    def sentclean(self,sent):   #去除标点等无用的符号
        self.sent = sent
        identify = str.maketrans('', '')
        delEStr = string.punctuation + string.digits      #ASCII 标点符号，数字  
        for sent in self.sent:
            cleansent = sent.translate(identify, delEStr) #去掉ASCII 标点符号和空格
        return cleansent

    def wordtoken(self,sent):     #分词     
        for sent in self.sent:
            self.word = nltk.word_tokenize(sent)
        return self.word

    def wordtag(self,word):        #标注词性
        self.tag = [nltk.pos_tag(word) for word in self.word]
        return self.tag

    def get_wordnet_pos(self,tag):  #通过tag获得pos
        if tag.startswith('J'):
            return nltk.corpus.wordnet.ADJ
        elif tag.startswith('V'):
            return nltk.corpus.wordnet.VERB
        elif tag.startswith('N'):
            return nltk.corpus.wordnet.NOUN
        elif tag.startswith('R'):
            return nltk.corpus.wordnet.ADV
        else:
            return ''
     def wordstem(self,word):   #词干提取
        lmtzr = WordNetLemmatizer()
        stemword = []
        for word in self.word:
            if word:
                tag = nltk.pos_tag(word_tokenize(word))   # tag is like [('bigger', 'JJR')]
                pos = get_wordnet_pos(tag[0][1])
                if pos:                # lemmatize()方法将word还原成pos词性的形式
                    lemmatized_word = lmtzr.lemmatize(word, pos)
                    stemword.append(lemmatized_word)
                else:
                    stemword.append(word)
        return stemword
    
     def wordclean(self,word):   #去除停用词(emoji什么的先不管后续有时间再处理)
        for word in self.word:
            cleanword = [word for word in self.word if word not in stopwords.words('english')]
        return cleanword

class operate:
    
    def _main_(self):   #过程整合
        data = usesql.getdata('select review from review21;')    #从mysql读取数据                                                   #分句
        sent = prepare.sentoken(data)           #分句
        cleansent = sentclean(sent)     #去除特殊符号
        word = wordtoken(cleansent)     #分词
        taggedword = wordtag(word)      #标注词性
        posedword = get_wordnet_pos(taggedword) #利用tag得到pos
        stemword = wordstem(posedword)                                                                      #去标点
        cleanword = wordclean(stemword)
        return cleanword          


cleanword = operate._main_()
output(cleanword)

print('Finished')
