import pymysql
import pandas as pd
import nltk
import string
import re
import sys
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

class usesql:


    def getdata(self,sql):   # 从mysql里读取数据
        conn = pymysql.connect(
            host = 'localhost',
            port = 3306,
            user = 'Leslie',
            password = 'hsh56912341',
            db = 'Yelp',
            charset = 'utf8'
        )
        cursor = conn.cursor()
        try:
            cursor.execute(sql)
            result = cursor.fetchmany(5)   #测试的时候先用fetchmany（）,正式运行时再改成fetchall
            return list(result)
            print("sql getdata")
        except:
            print("Error: unable to fetch data")
        
         

    def output(self,cleanwords):
        conn = pymysql.connect(
            host = 'localhost',
            port = 3306,
            user = 'Leslie',
            password = 'hsh56912341',
            db = 'Yelp',
            charset = 'utf8'
        )
        cursor = conn.cursor()
        df = pd.DataFrame (list(cleanwords))
        df.to_sql(              #将输出结果转为dataframe格式存入mysql，表的名称为测试阶段暂定，正式名称最后命名为reviewdata
            name='firstest',
            conn=conn,
            if_exists='append',
            index=False,
        )
        cursor.close()
        conn.close()
        print("sql output")
        

class prepare:

    def sentoken(self,data):   #分句
        token = nltk.data.load('tokenizers/punkt/english.pickle')
        sents = token.tokenize(str(data))
        return sents
        
    def sentclean(self,sents):   #去除标点等无用的符号
        delEStr = string.punctuation + string.digits
        DelEDict = {}
        for ch in delEStr:
            DelEDict[ch] = ''
        trans = str.maketrans(DelEDict)

        cleansents = []
        for sent in sents:
            cleansent = sent.translate(trans) #去掉ASCII标点符号
            cleansents.append(cleansent)
        return cleansents

    def wordtoken(self,cleansents):
        words = []    #分词     
        for cleansent in cleansents:
            word = nltk.word_tokenize(cleansent)
            words.append(word)
        return words

    def wordtag(self,words): #标注词性
        word_tags = []   
        for word in words:    
            word_tags.extend(nltk.pos_tag(word)) 
        return word_tags

    def get_wordnet_pos(self,word_tags):  #通过tag获得pos
        word_poses = []
        for word_tag in word_tags:        #此处的 word_tag is like ('bigger', 'JJR')
            if word_tag[1].startswith('J'):
                pos = nltk.corpus.wordnet.ADJ
            elif word_tag[1].startswith('V'):
                pos = nltk.corpus.wordnet.VERB
            elif word_tag[1].startswith('N'):
                pos = nltk.corpus.wordnet.NOUN
            elif word_tag[1].startswith('R'):
                pos = nltk.corpus.wordnet.ADV        
            else:
                pos = ''
            word_poses.append([(word_tag[0],pos)])
        return word_poses

   
    def wordstem(self,word_poses):   
        lmtzr = WordNetLemmatizer()
        stemwords = []
        for word_pos in word_poses:
            if word_pos:                                # lemmatize()方法将word还原成pos词性的形式
                lemmatized_word = lmtzr.lemmatize(word_pos[0])  
                stemwords.append(lemmatized_word)
            else:
                stemwords.append(word_pos[0][0])      # word_pos = [(word_tag[0],pos)]
        return stemwords
   
    def wordclean(self,stemwords):   #去除停用词(emoji什么的先不管后续有时间再处理)
        cleanwords = []
        for stemword in stemwords:
            if stemword not in stopwords.words('english'):
               cleanwords.append(stemword)
        return cleanwords

class operate:
    
    def _main_(self):   #过程整合
        sql = usesql()
        data = sql.getdata('select review from review21;')  #从mysql读取数据                                                  
        prep = prepare()
        sents = prep.sentoken(data)           #分句
        print(sents)
        print('\n')
        cleansents = prep.sentclean(sents)    #去除特殊符号
        print(cleansents) 
        print('\n')    
        words = prep.wordtoken(cleansents) 
        print(words)
        print('\n')    #分词
        word_tags = prep.wordtag(words)      #标注词性
        print(word_tags)
        print('\n')
        word_poses = prep.get_wordnet_pos(word_tags) #利用tag得到pos
        print(word_poses)
        print('\n')
        stemwords = prep.wordstem(word_poses)          #词形还原                                                            
        print(stemwords)
        print('\n')
        cleanwords = prep.wordclean(stemwords)         #去停用词
        print(cleanwords)
        print('\n')
        return cleanwords          

op = operate()
cleanwords = op._main_()

sql = usesql()        #导出清洗后的数据到mysql
sql.output(cleanwords)

print('Finished')
