import nltk
import string
import re
import sys
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

class prepare:

    def _init_(self,word,relativity,id,ordinal):
        self.word = word
        self.relativity = relativity
        self.id = id
        self.ordinal = ordinal
        
    def sentoken(self,data):   #分句
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        sent = sent_tokenizer.tokenize(str(data))
        
    def sentclean(self,sent):   #去除标点等无用的符号
        self.sent = sent
        identify = str.maketrans('', '')
        delEStr = string.punctuation + string.digits      #ASCII 标点符号，数字  
        for sent in self.sent:
            cleansent = sent.translate(identify, delEStr) #去掉ASCII 标点符号和空格
        return cleansent

    def wordtoken(self):     #分词     
        for sent in self.sent:
            self.word = nltk.word_tokenize(sent)
        return self.word

    def wordtag(self):        #标注词性
        self.tag = [nltk.pos_tag(word) for word in self.word]
     

    def wordclean(self):   #去除停用词(emoji什么的先不管后续有时间再处理)
        for word in self.word:
            cleanwords = [word for word in self.word if word not in stopwords.words('english')]
        return cleanwords
    


    
    '''lemmatize这一步骤以下两种写法应该用哪个还没确定'''
    
    def wordlemma(self):      #通过tag获得pos
        words_lematizer = []
        wordnet_lematizer = WordNetLemmatizer()
        for word, tag in self.word:
            if tag.startswith('NN'):
                word_lematizer =  wordnet_lematizer.lemmatize(word, pos='n')  # n代表名词
            elif tag.startswith('VB'): 
                word_lematizer =  wordnet_lematizer.lemmatize(word, pos='v')   # v代表动词
            elif tag.startswith('JJ'): 
                word_lematizer =  wordnet_lematizer.lemmatize(word, pos='a')   # a代表形容词
            elif tag.startswith('R'): 
                word_lematizer =  wordnet_lematizer.lemmatize(word, pos='r')   # r代表副词
            else: 
                word_lematizer =  wordnet_lematizer.lemmatize(word)
            words_lematizer.append(word_lematizer)
    def wordstem(self):       #词形还原（词干提取）
        wordnet_lematizer = WordNetLemmatizer()
        lemmawords = [wordnet_lematizer.lemmatize(word) for word in self.word]

    '''or'''

    def get_wordnet_pos(self,tag):  #通过tag获得pos
        if tag.startswith('J'):
            return nltk.corpus.wordnet.ADJ
        elif tag.startswith('V'):
            return nltk.corpus.wordnet.VERB
        elif tag.startswith('N'):
            return nltk.corpus.wordnet.NOUN
        elif tag.startswith('R'):
            return nltk.corpus.wordnet.ADV
        else:
            return ''
     def wordstem(self):   #词干提取
        lmtzr = WordNetLemmatizer()
        stemwords = []
        for word in self.word:
            if word:
                tag = nltk.pos_tag(word_tokenize(word))   # tag is like [('bigger', 'JJR')]
                pos = get_wordnet_pos(tag[0][1])
                if pos:                # lemmatize()方法将word还原成pos词性的形式
                    lemmatized_word = lmtzr.lemmatize(word, pos)
                    stemwords.append(lemmatized_word)
                else:
                    stemwords.append(word)
        return stemwords
   
   
   
    '''去标点符号的另一种写法'''

    
    def sentclean(self):
        characters = [',', '.','DBSCAN', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%','-','...','^','{','}']
        wordlist = [word for word in self.word if word not in characters]

